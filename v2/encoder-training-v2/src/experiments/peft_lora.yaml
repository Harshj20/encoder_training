model:
  name_or_path: "intfloat/multilingual-e5-small"
  tokenizer: null
  pooling:
    mode: "mean"
    layers: [-1]
    normalize: true
  classifier_hidden: null  # For embeddings, no classifier needed
  num_labels: 2
  dropout: 0.1

training:
  mode: "peft"  # PEFT with LoRA
  epochs: 5
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 32
  gradient_accumulation_steps: 4
  fp16: true
  bf16: false
  max_seq_length: 256
  optimizer: "adamw"
  lr: 5e-5
  weight_decay: 0.01
  scheduler: "linear"
  warmup_ratio: 0.1
  max_grad_norm: 1.0
  seed: 42
  logging_steps: 10
  eval_steps: 1000
  save_steps: 1000
  save_total_limit: 3
  output_dir: "./outputs/peft_lora"

data:
  type: "pair"  # text1, text2, label
  dataset: "examples/sample_dataset.csv"
  split: "train"
  streaming: false
  text1_field: "text1"
  text2_field: "text2"
  label_field: "label"
  max_samples: null

peft:
  enabled: true
  lora_rank: 8
  lora_alpha: 32
  lora_dropout: 0.1
  target_modules: null  # Auto-detect

deepspeed:
  enabled: false

loss:
  type: "classification"  # Can also use "contrastive"
  temperature: 0.07
  multi_label: false
