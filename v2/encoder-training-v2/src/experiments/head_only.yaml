model:
  name_or_path: "intfloat/multilingual-e5-small"
  tokenizer: null  # Use same as model
  pooling:
    mode: "mean"
    layers: [-1]
    normalize: true
  classifier_hidden: 512
  num_labels: 2
  dropout: 0.1

training:
  mode: "head_only"  # Freeze encoder, train head only
  epochs: 3
  per_device_train_batch_size: 16
  per_device_eval_batch_size: 32
  gradient_accumulation_steps: 2
  fp16: true
  bf16: false
  max_seq_length: 256
  optimizer: "adamw"
  lr: 1e-3  # Higher learning rate for head only
  weight_decay: 0.01
  scheduler: "linear"
  warmup_ratio: 0.1
  max_grad_norm: 1.0
  seed: 42
  logging_steps: 10
  eval_steps: 500
  save_steps: 500
  save_total_limit: 3
  output_dir: "./outputs/head_only"

data:
  type: "single"
  dataset: "examples/sample_dataset.csv"
  split: "train"
  streaming: false  # Small dataset, load in memory
  text_field: "text"
  label_field: "label"
  max_samples: null

peft:
  enabled: false

deepspeed:
  enabled: false

loss:
  type: "classification"
  multi_label: false
